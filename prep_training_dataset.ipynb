{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is meant to take the papers and create a training dataset of \n",
    "1) research goal prompt\n",
    "2) research hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from llm import complete_text_openai\n",
    "\n",
    "# Folder path and maximum characters per file\n",
    "logs_folder_path = 'autoscious_logs/'\n",
    "MAX_CHARS_PER_PAPER = 50000 # 12500 tokens. GPT-3.5-Turbo-1106 has max context window of 16K, max output tokens of 4K.\n",
    "MIN_CHARS_PER_PAPER = 500 # There's a decent amount of just errors in the text files, so normal and meaningful papers will be above this\n",
    "\n",
    "# List to store file contents\n",
    "file_contents = []\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for paper_file in os.listdir(logs_folder_path):\n",
    "    file_path = os.path.join(logs_folder_path, paper_file)\n",
    "\n",
    "    # Check if it's a file\n",
    "    if os.path.isfile(file_path):\n",
    "        # Read the file's content\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        # Check if the content is long enough\n",
    "        if len(content) < MIN_CHARS_PER_PAPER:\n",
    "            continue\n",
    "\n",
    "        # Truncate the content if it exceeds the limit\n",
    "        if len(content) > MAX_CHARS_PER_PAPER:\n",
    "            content = content[:MAX_CHARS_PER_PAPER]\n",
    "\n",
    "        # Append the content to the list\n",
    "        file_contents.append(content)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(file_contents, columns=['paper_full_text'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('autoscious_logs_full-text.csv', index=False)\n",
    "print(\"Number of papers: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 0\n",
      "Processing index: 1\n",
      "Processing index: 2\n",
      "Processing index: 3\n",
      "Processing index: 4\n",
      "Processing index: 5\n",
      "Processing index: 6\n",
      "Processing index: 7\n",
      "Processing index: 8\n",
      "Processing index: 9\n",
      "Processing index: 10\n",
      "Processing index: 11\n",
      "Processing index: 12\n",
      "Processing index: 13\n",
      "Processing index: 14\n",
      "Processing index: 15\n",
      "Processing index: 16\n",
      "Processing index: 17\n",
      "Processing index: 18\n",
      "Processing index: 19\n",
      "Processing index: 20\n",
      "Exception:  Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 16461 tokens (14461 in the messages, 2000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Processing index: 21\n",
      "Processing index: 22\n",
      "Processing index: 23\n",
      "Processing index: 24\n",
      "Processing index: 25\n",
      "Processing index: 26\n",
      "Processing index: 27\n",
      "Processing index: 28\n",
      "Processing index: 29\n",
      "Processing index: 30\n",
      "Exception:  Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 17634 tokens (15634 in the messages, 2000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Processing index: 31\n",
      "Processing index: 32\n",
      "Processing index: 33\n",
      "Processing index: 34\n",
      "Exception:  Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 16472 tokens (14472 in the messages, 2000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Processing index: 35\n",
      "Processing index: 36\n",
      "Processing index: 37\n",
      "Processing index: 38\n",
      "Processing index: 39\n",
      "Processing index: 40\n",
      "Processing index: 41\n",
      "Processing index: 42\n",
      "Processing index: 43\n",
      "Processing index: 44\n",
      "Processing index: 45\n",
      "Processing index: 46\n",
      "Processing index: 47\n",
      "Processing index: 48\n",
      "Processing index: 49\n",
      "Exception:  Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 16588 tokens (14588 in the messages, 2000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Processing index: 50\n",
      "Processing index: 51\n",
      "Processing index: 52\n",
      "Processing index: 53\n",
      "Processing index: 54\n",
      "Processing index: 55\n",
      "Processing index: 56\n",
      "Processing index: 57\n",
      "Processing index: 58\n",
      "Processing index: 59\n",
      "Processing index: 60\n",
      "Processing index: 61\n",
      "Exception:  Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 16713 tokens (14713 in the messages, 2000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Processing index: 62\n",
      "Processing index: 63\n",
      "Processing index: 64\n",
      "Processing index: 65\n",
      "Processing index: 66\n",
      "Processing index: 67\n",
      "Processing index: 68\n",
      "Exception:  Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 17074 tokens (15074 in the messages, 2000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Processing index: 69\n",
      "Processing index: 70\n"
     ]
    }
   ],
   "source": [
    "# Create research goal for each paper\n",
    "def create_research_goal_prompt(full_paper_text):\n",
    "    try:\n",
    "        return complete_text_openai(f'''You will help me create a training dataset for generating a research goal prompt based on a research paper text. Try your best to determine what the starting research goal was before the project approach was determined and executed on.\n",
    "\n",
    "Research paper text: {full_paper_text}\n",
    "\n",
    "Return only the research goal prompt in the format \"Propose a reasonable hypothesis about how to <insert research goal>\". Do not respond with any conversation or explanation. If you cannot write a research goal prompt because there doesn't seem to be a research goal, just respond with \"N/A\".''')\n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", e)\n",
    "        return 'N/A'\n",
    "    \n",
    "# Read the existing DataFrame\n",
    "df = pd.read_csv('autoscious_logs_full-text.csv')\n",
    "\n",
    "# Before applying, add mechanism in case it fails to pick back up\n",
    "# Add a column for status if it doesn't exist\n",
    "if 'Status' not in df.columns:\n",
    "    df['Status'] = 'Not Processed'\n",
    "\n",
    "# Add a column for the research goal if it doesn't exist\n",
    "if 'research_goal' not in df.columns:\n",
    "    df['research_goal'] = None\n",
    "\n",
    "# Apply the function to each row in the 'paper_full_text' column\n",
    "for index, row in df.iterrows():\n",
    "    if row['Status'] == 'Not Processed':\n",
    "        try:\n",
    "            # Print the current index\n",
    "            print(f'Processing index: {index}')\n",
    "\n",
    "            # Apply the function\n",
    "            result = create_research_goal_prompt(row['paper_full_text'])\n",
    "            df.at[index, 'research_goal'] = result\n",
    "            df.at[index, 'Status'] = 'Processed'\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error at index {index}: {e}')\n",
    "            df.at[index, 'Status'] = 'Failed'\n",
    "\n",
    "        # Save progress intermittently\n",
    "        if index % 20 == 0:\n",
    "            df.to_csv('autoscious_logs_progress_full-text_goal.csv', index=False)\n",
    "\n",
    "# Save the updated DataFrame back to CSV\n",
    "df.drop(columns=['Status'], inplace=True)\n",
    "df.to_csv('autoscious_logs_complete_full-text_goal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_full_text</th>\n",
       "      <th>research_goal</th>\n",
       "      <th>research_hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Affinity Improvement of a Therapeutic Antibody...</td>\n",
       "      <td>Propose a reasonable hypothesis about how to o...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We use essential cookies to make sure the site...</td>\n",
       "      <td>Propose a reasonable hypothesis about how to i...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An Immunogenic Personal Neoantigen Vaccine for...</td>\n",
       "      <td>Propose a reasonable hypothesis about how to g...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1\\nSCieNTifiC  REPORTS  |  (2018) 8:2260  | DO...</td>\n",
       "      <td>Propose a reasonable hypothesis about how to i...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Application of PBPK modeling to predict monocl...</td>\n",
       "      <td>Propose a reasonable hypothesis about how to p...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     paper_full_text  \\\n",
       "0  Affinity Improvement of a Therapeutic Antibody...   \n",
       "1  We use essential cookies to make sure the site...   \n",
       "2  An Immunogenic Personal Neoantigen Vaccine for...   \n",
       "3  1\\nSCieNTifiC  REPORTS  |  (2018) 8:2260  | DO...   \n",
       "4  Application of PBPK modeling to predict monocl...   \n",
       "\n",
       "                                       research_goal research_hypothesis  \n",
       "0  Propose a reasonable hypothesis about how to o...                None  \n",
       "1  Propose a reasonable hypothesis about how to i...                None  \n",
       "2  Propose a reasonable hypothesis about how to g...                None  \n",
       "3  Propose a reasonable hypothesis about how to i...                None  \n",
       "4  Propose a reasonable hypothesis about how to p...                None  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 0\n",
      "Processing index: 1\n",
      "Processing index: 2\n",
      "Processing index: 3\n",
      "Processing index: 4\n",
      "Processing index: 5\n",
      "Processing index: 6\n",
      "Processing index: 7\n",
      "Processing index: 8\n",
      "Processing index: 9\n",
      "Processing index: 10\n",
      "Processing index: 11\n",
      "Processing index: 12\n",
      "Processing index: 13\n",
      "Processing index: 14\n",
      "Processing index: 15\n",
      "Processing index: 16\n",
      "Processing index: 17\n",
      "Processing index: 18\n",
      "Processing index: 19\n",
      "Processing index: 20\n",
      "Exception:  Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 16467 tokens (14467 in the messages, 2000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Processing index: 21\n",
      "Processing index: 22\n",
      "Processing index: 23\n",
      "Processing index: 24\n",
      "Processing index: 25\n",
      "Processing index: 26\n",
      "Processing index: 27\n",
      "Processing index: 28\n",
      "Processing index: 29\n",
      "Processing index: 30\n",
      "Exception:  Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 17640 tokens (15640 in the messages, 2000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Processing index: 31\n",
      "Processing index: 32\n",
      "Processing index: 33\n",
      "Processing index: 34\n",
      "Exception:  Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 16478 tokens (14478 in the messages, 2000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Processing index: 35\n",
      "Processing index: 36\n",
      "Processing index: 37\n",
      "Processing index: 38\n",
      "Processing index: 39\n",
      "Processing index: 40\n",
      "Processing index: 41\n",
      "Processing index: 42\n",
      "Processing index: 43\n",
      "Processing index: 44\n",
      "Processing index: 45\n",
      "Processing index: 46\n",
      "Processing index: 47\n",
      "Processing index: 48\n",
      "Processing index: 49\n",
      "Exception:  Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 16594 tokens (14594 in the messages, 2000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Processing index: 50\n",
      "Processing index: 51\n",
      "Processing index: 52\n",
      "Processing index: 53\n",
      "Processing index: 54\n",
      "Processing index: 55\n",
      "Processing index: 56\n",
      "Processing index: 57\n",
      "Processing index: 58\n",
      "Processing index: 59\n",
      "Processing index: 60\n",
      "Processing index: 61\n",
      "Exception:  Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 16719 tokens (14719 in the messages, 2000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Processing index: 62\n",
      "Processing index: 63\n",
      "Processing index: 64\n",
      "Processing index: 65\n",
      "Processing index: 66\n",
      "Processing index: 67\n",
      "Processing index: 68\n",
      "Exception:  Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 17080 tokens (15080 in the messages, 2000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Processing index: 69\n",
      "Processing index: 70\n"
     ]
    }
   ],
   "source": [
    "# Create research hypothesis and proposal for each paper and research goal\n",
    "def create_research_hypothesis_proposal_prompt(full_paper_text, research_goal):\n",
    "    try:\n",
    "        return complete_text_openai(f'''You will help me create a training dataset for generating a research hypothesis and proposal based on a research paper text and its corresponding research goal. Try your best to determine what the initial research hypothesis and proposal was after the research goal was determined, but before the project approach was determined.\n",
    "\n",
    "Research goal: {research_goal}\n",
    "\n",
    "Research paper text: {full_paper_text}\n",
    "\n",
    "Return only the research hypothesis and proposal. Do not respond with any conversation. If you cannot write a research hypothesis and proposal because there doesn't seem to be a research goal or research paper text, just respond with \"N/A\".''')\n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", e)\n",
    "        return 'N/A'\n",
    "    \n",
    "# Read the existing DataFrame\n",
    "df = pd.read_csv('autoscious_logs_complete_full-text_goal.csv')\n",
    "\n",
    "# Before applying, add mechanism in case it fails to pick back up\n",
    "# Add a column for status if it doesn't exist\n",
    "if 'Status' not in df.columns:\n",
    "    df['Status'] = 'Not Processed'\n",
    "\n",
    "# Add a column for the research goal if it doesn't exist\n",
    "if 'research_hypothesis' not in df.columns:\n",
    "    df['research_hypothesis'] = None\n",
    "\n",
    "# Apply the function to each row in the 'paper_full_text' column\n",
    "for index, row in df.iterrows():\n",
    "    if row['Status'] == 'Not Processed':\n",
    "        try:\n",
    "            # Print the current index\n",
    "            print(f'Processing index: {index}')\n",
    "\n",
    "            # Apply the function\n",
    "            result = create_research_hypothesis_proposal_prompt(row['paper_full_text'], row['research_goal'])\n",
    "            df.at[index, 'research_hypothesis'] = result\n",
    "            df.at[index, 'Status'] = 'Processed'\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error at index {index}: {e}')\n",
    "            df.at[index, 'Status'] = 'Failed'\n",
    "\n",
    "        # Save progress intermittently\n",
    "        if index % 20 == 0:\n",
    "            df.to_csv('autoscious_logs_progress_full-text_goal_hypothesis.csv', index=False)\n",
    "\n",
    "# Save the updated DataFrame back to CSV\n",
    "df.drop(columns=['Status'], inplace=True)\n",
    "df.to_csv('autoscious_logs_complete_full-text_goal_hypothesis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with N/A before fine-tuning\n",
    "import numpy as np\n",
    "\n",
    "# Read the existing DataFrame\n",
    "df = pd.read_csv('autoscious_logs_complete_full-text_goal_hypothesis.csv')\n",
    "\n",
    "# Replace 'N/A' strings with NaN\n",
    "df = df.replace('N/A', np.nan)\n",
    "\n",
    "# Drop any rows with NaN\n",
    "df = df.dropna()\n",
    "\n",
    "df.to_csv('autoscious_logs_complete_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_full_text</th>\n",
       "      <th>research_goal</th>\n",
       "      <th>research_hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Affinity Improvement of a Therapeutic Antibody...</td>\n",
       "      <td>Propose a reasonable hypothesis about how to o...</td>\n",
       "      <td>Research Hypothesis: Introducing charged resid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An Immunogenic Personal Neoantigen Vaccine for...</td>\n",
       "      <td>Propose a reasonable hypothesis about how to g...</td>\n",
       "      <td>Research hypothesis: Vaccination with neoantig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1\\nSCieNTifiC  REPORTS  |  (2018) 8:2260  | DO...</td>\n",
       "      <td>Propose a reasonable hypothesis about how to i...</td>\n",
       "      <td>Research Hypothesis:\\nThe affinity maturation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Application of PBPK modeling to predict monocl...</td>\n",
       "      <td>Propose a reasonable hypothesis about how to p...</td>\n",
       "      <td>Research Hypothesis:\\nThe disposition of monoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nucleic acids can not only hybridize to one an...</td>\n",
       "      <td>Propose a reasonable hypothesis about how to d...</td>\n",
       "      <td>Research hypothesis: We hypothesize that by us...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     paper_full_text  \\\n",
       "0  Affinity Improvement of a Therapeutic Antibody...   \n",
       "2  An Immunogenic Personal Neoantigen Vaccine for...   \n",
       "3  1\\nSCieNTifiC  REPORTS  |  (2018) 8:2260  | DO...   \n",
       "4  Application of PBPK modeling to predict monocl...   \n",
       "5  Nucleic acids can not only hybridize to one an...   \n",
       "\n",
       "                                       research_goal  \\\n",
       "0  Propose a reasonable hypothesis about how to o...   \n",
       "2  Propose a reasonable hypothesis about how to g...   \n",
       "3  Propose a reasonable hypothesis about how to i...   \n",
       "4  Propose a reasonable hypothesis about how to p...   \n",
       "5  Propose a reasonable hypothesis about how to d...   \n",
       "\n",
       "                                 research_hypothesis  \n",
       "0  Research Hypothesis: Introducing charged resid...  \n",
       "2  Research hypothesis: Vaccination with neoantig...  \n",
       "3  Research Hypothesis:\\nThe affinity maturation ...  \n",
       "4  Research Hypothesis:\\nThe disposition of monoc...  \n",
       "5  Research hypothesis: We hypothesize that by us...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
