{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is meant to take the papers and create a training dataset of \n",
    "1) research goal prompt\n",
    "2) research hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from llm import complete_text_openai\n",
    "\n",
    "# Folder path and maximum characters per file\n",
    "logs_folder_path = 'autoscious_logs/'\n",
    "MAX_CHARS_PER_PAPER = 50000 # 12500 tokens. GPT-3.5-Turbo-1106 has max context window of 16K, max output tokens of 4K.\n",
    "MIN_CHARS_PER_PAPER = 500 # There's a decent amount of just errors in the text files, so normal and meaningful papers will be above this\n",
    "\n",
    "# List to store file contents\n",
    "file_contents = []\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for paper_file in os.listdir(logs_folder_path):\n",
    "    file_path = os.path.join(logs_folder_path, paper_file)\n",
    "\n",
    "    # Check if it's a file\n",
    "    if os.path.isfile(file_path):\n",
    "        # Read the file's content\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        # Check if the content is long enough\n",
    "        if len(content) < MIN_CHARS_PER_PAPER:\n",
    "            continue\n",
    "\n",
    "        # Truncate the content if it exceeds the limit\n",
    "        if len(content) > MAX_CHARS_PER_PAPER:\n",
    "            content = content[:MAX_CHARS_PER_PAPER]\n",
    "\n",
    "        # Append the content to the list\n",
    "        file_contents.append(content)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(file_contents, columns=['paper_full_text'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('autoscious_logs_full-text.csv', index=False)\n",
    "print(\"Number of papers: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 0\n",
      "Processing index: 1\n",
      "Processing index: 2\n",
      "Processing index: 3\n",
      "Processing index: 4\n",
      "Processing index: 5\n",
      "Processing index: 6\n",
      "Processing index: 7\n",
      "Processing index: 8\n",
      "Processing index: 9\n",
      "Processing index: 10\n",
      "Processing index: 11\n",
      "Processing index: 12\n",
      "Processing index: 13\n",
      "Processing index: 14\n",
      "Processing index: 15\n",
      "Processing index: 16\n",
      "Processing index: 17\n",
      "Processing index: 18\n",
      "Processing index: 19\n",
      "Processing index: 20\n",
      "Processing index: 21\n",
      "Processing index: 22\n",
      "Processing index: 23\n",
      "Processing index: 24\n",
      "Processing index: 25\n",
      "Processing index: 26\n",
      "Processing index: 27\n",
      "Processing index: 28\n",
      "Processing index: 29\n",
      "Processing index: 30\n",
      "Error at index 30: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 17530 tokens (15530 in the messages, 2000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Processing index: 31\n",
      "Processing index: 32\n",
      "Processing index: 33\n",
      "Processing index: 34\n",
      "Processing index: 35\n",
      "Processing index: 36\n",
      "Processing index: 37\n",
      "Processing index: 38\n",
      "Processing index: 39\n",
      "Processing index: 40\n",
      "Processing index: 41\n",
      "Processing index: 42\n",
      "Processing index: 43\n",
      "Processing index: 44\n",
      "Processing index: 45\n",
      "Processing index: 46\n",
      "Processing index: 47\n",
      "Processing index: 48\n",
      "Processing index: 49\n",
      "Error at index 49: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 16482 tokens (14482 in the messages, 2000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Processing index: 50\n",
      "Processing index: 51\n",
      "Processing index: 52\n",
      "Processing index: 53\n",
      "Processing index: 54\n",
      "Processing index: 55\n",
      "Processing index: 56\n",
      "Processing index: 57\n",
      "Processing index: 58\n",
      "Processing index: 59\n",
      "Processing index: 60\n",
      "Processing index: 61\n",
      "Error at index 61: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 16608 tokens (14608 in the messages, 2000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Processing index: 62\n",
      "Processing index: 63\n",
      "Processing index: 64\n",
      "Processing index: 65\n",
      "Processing index: 66\n",
      "Processing index: 67\n",
      "Processing index: 68\n",
      "Error at index 68: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 16969 tokens (14969 in the messages, 2000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Processing index: 69\n",
      "Processing index: 70\n"
     ]
    }
   ],
   "source": [
    "# Create research goal for each paper\n",
    "def create_research_goal_prompt(full_paper_text):\n",
    "    try:\n",
    "        return complete_text_openai(f'''You will help me create a training dataset for generating a research goal prompt based on a research paper text. Try your best to determine what the starting research goal was before the project approach was determined and executed on.\n",
    "\n",
    "Research paper text: {full_paper_text}\n",
    "\n",
    "Return only the research goal prompt in the format \"Propose a reasonable hypothesis about how to <insert research goal>\". Do not respond with any conversation or explanation. If you cannot write a research goal prompt because there doesn't seem to be a research goal, just respond with \"N/A\".''')\n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", e)\n",
    "        return 'N/A'\n",
    "    \n",
    "# Read the existing DataFrame\n",
    "df = pd.read_csv('autoscious_logs_full-text.csv')\n",
    "\n",
    "# Before applying, add mechanism in case it fails to pick back up\n",
    "# Add a column for status if it doesn't exist\n",
    "if 'Status' not in df.columns:\n",
    "    df['Status'] = 'Not Processed'\n",
    "\n",
    "# Add a column for the research goal if it doesn't exist\n",
    "if 'research_goal' not in df.columns:\n",
    "    df['research_goal'] = None\n",
    "\n",
    "# Apply the function to each row in the 'paper_full_text' column\n",
    "for index, row in df.iterrows():\n",
    "    if row['Status'] == 'Not Processed':\n",
    "        try:\n",
    "            # Print the current index\n",
    "            print(f'Processing index: {index}')\n",
    "\n",
    "            # Apply the function\n",
    "            result = complete_text_openai(row['paper_full_text'])\n",
    "            df.at[index, 'research_goal'] = result\n",
    "            df.at[index, 'Status'] = 'Processed'\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error at index {index}: {e}')\n",
    "            df.at[index, 'Status'] = 'Failed'\n",
    "\n",
    "        # Save progress intermittently\n",
    "        if index % 20 == 0:\n",
    "            df.to_csv('autoscious_logs_progress_full-text_goal.csv', index=False)\n",
    "\n",
    "# Save the updated DataFrame back to CSV\n",
    "df.to_csv('autoscious_logs_complete_full-text_goal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
